{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1LPGWUALWF3j6PTSIjQl1DimsfYgrtDxJ",
      "authorship_tag": "ABX9TyNtizP2LBNzSZ0kiPej4N/Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffvun/Jarvis-Assisant/blob/master/CNN_models_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading necessary libraries**"
      ],
      "metadata": {
        "id": "go8s0gsiTO2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit-keras\n",
        "!pip install tensorflow-addons\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDN9bYkpXEvA",
        "outputId": "32ded827-b10e-4d8a-e7df-b361151ba451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vit-keras\n",
            "  Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit-keras) (1.11.3)\n",
            "Collecting validators (from vit-keras)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit-keras) (1.23.5)\n",
            "Installing collected packages: validators, vit-keras\n",
            "Successfully installed validators-0.22.0 vit-keras-0.1.2\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.3/612.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.22.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrUYgap3yiYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e653b6a8-1295-488d-86f9-d3573bc3a309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from vit_keras import vit, utils\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**"
      ],
      "metadata": {
        "id": "JbraZ48RTYwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "NUM_CLASSES = 4\n"
      ],
      "metadata": {
        "id": "uDNJshJ8yotN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data directories and categories\n",
        "data_dir = '/content/drive/MyDrive/DataSet/train/ALL'\n",
        "categories = ['Benign', 'Early', 'Pre', 'Pro']\n",
        "\n",
        "# Create an empty DataFrame to store file paths and corresponding labels\n",
        "data = []\n",
        "\n",
        "# Load data by iterating through categories and collecting file paths\n",
        "for category_id, category in enumerate(categories):\n",
        "    category_dir = os.path.join(data_dir, category)\n",
        "    for filename in os.listdir(category_dir):\n",
        "        if filename.endswith('.jpg'):\n",
        "          data.append((os.path.join(category_dir, filename), category))"
      ],
      "metadata": {
        "id": "zxdqgPrW-Qud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data list into a DataFrame\n",
        "data_df = pd.DataFrame(data, columns=['FilePath', 'Category'])"
      ],
      "metadata": {
        "id": "RnF8UApbCi4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training, testing, and validation sets\n",
        "train_data, test_data = train_test_split(data_df, test_size=0.2, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
        "\n",
        "# Print the number of samples in each set\n",
        "print(f\"Number of samples in training set: {len(train_data)}\")\n",
        "print(f\"Number of samples in testing set: {len(test_data)}\")\n",
        "print(f\"Number of samples in validation set: {len(val_data)}\")\n"
      ],
      "metadata": {
        "id": "A4zPANws4ZPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc24eee0-d581-47d9-f1ef-c9079b4f68d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in training set: 2343\n",
            "Number of samples in testing set: 652\n",
            "Number of samples in validation set: 261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data generators\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "5iANrYLCyzRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=train_data,\n",
        "    x_col=\"FilePath\",\n",
        "    y_col=\"Category\",\n",
        "    target_size=(IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=val_data,\n",
        "    x_col=\"FilePath\",\n",
        "    y_col=\"Category\",\n",
        "    target_size=(IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=test_data,\n",
        "    x_col=\"FilePath\",\n",
        "    y_col=\"Category\",\n",
        "    target_size=(IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "id": "oTbgks8Ey51P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf369b1-aead-4c88-a6ba-fea3935f1015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2343 validated image filenames belonging to 4 classes.\n",
            "Found 261 validated image filenames belonging to 4 classes.\n",
            "Found 652 validated image filenames belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading and Compiling Various CNN models**"
      ],
      "metadata": {
        "id": "jVNjIFfLTzVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and compile SVM model\n",
        "# Flatten the images for SVM\n",
        "X_svm, y_svm = [], []\n",
        "\n",
        "for i in range(len(train_generator)):\n",
        "    batch_images, batch_labels = train_generator[i]\n",
        "    batch_size = batch_images.shape[0]\n",
        "    flattened_images = batch_images.reshape((batch_size, -1))\n",
        "    X_svm.append(flattened_images)\n",
        "    y_svm.append(np.argmax(batch_labels, axis=1))\n",
        "\n",
        "X_svm = np.vstack(X_svm)\n",
        "y_svm = np.concatenate(y_svm)\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_svm, y_svm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "0yHKDsMhJFWK",
        "outputId": "444f3058-79fc-46b4-b7f9-a309e918fd79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and compile CNN model\n",
        "cnn_model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(*IMAGE_SIZE, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "5mxq8umjLTW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and compile ViT model\n",
        "vit_model = vit.vit_b16(\n",
        "    image_size=IMAGE_SIZE[0],\n",
        "    activation='softmax',\n",
        "    pretrained=False,\n",
        "    include_top=True,\n",
        "    pretrained_top=False,\n",
        "    classes=4\n",
        ")\n",
        "\n",
        "vit_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "val0a_W8LN8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and compile ResNet model\n",
        "resnet_model = ResNet50(\n",
        "    include_top=True,\n",
        "    weights=None,\n",
        "    input_shape=(*IMAGE_SIZE, 3),\n",
        "    classes=4\n",
        ")\n",
        "\n",
        "resnet_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "oj_GGC6BLcdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and configure MobileNetV2 model\n",
        "base_model = MobileNetV2(\n",
        "    input_shape=(*IMAGE_SIZE, 3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        ")\n",
        "#Adding custom classification layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "mobilenet_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "# Compile the model\n",
        "mobilenet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "NTc2h5F5y9c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cb41d3-c965-4b0a-d6f6-0880a3cbc391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training and Evaluating Various CNN Models**"
      ],
      "metadata": {
        "id": "c8OVBSFQT_-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train CNN, ViT, ResNet and mobilenet models\n",
        "\n",
        "history_cnn = cnn_model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, verbose=1)\n",
        "history_vit = vit_model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, verbose=1)\n",
        "history_resnet = resnet_model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, verbose=1)\n",
        "history_mobilenet = mobilenet_model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, verbose=1)\n"
      ],
      "metadata": {
        "id": "V9kVChrLzOJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596c09ee-61bc-48f4-88b1-1ec7b855b225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "74/74 [==============================] - 196s 3s/step - loss: 9.0506 - accuracy: 0.5809 - val_loss: 0.6708 - val_accuracy: 0.7854\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 115s 2s/step - loss: 0.6073 - accuracy: 0.7738 - val_loss: 0.5025 - val_accuracy: 0.7969\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 113s 2s/step - loss: 0.5369 - accuracy: 0.7870 - val_loss: 0.5429 - val_accuracy: 0.7356\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 114s 2s/step - loss: 0.5204 - accuracy: 0.7870 - val_loss: 0.4607 - val_accuracy: 0.7893\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 112s 2s/step - loss: 0.4665 - accuracy: 0.8229 - val_loss: 0.5061 - val_accuracy: 0.7931\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 120s 2s/step - loss: 0.4325 - accuracy: 0.8344 - val_loss: 0.5007 - val_accuracy: 0.8046\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 111s 1s/step - loss: 0.4038 - accuracy: 0.8438 - val_loss: 0.4679 - val_accuracy: 0.8429\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 114s 2s/step - loss: 0.3952 - accuracy: 0.8438 - val_loss: 0.3845 - val_accuracy: 0.8544\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 119s 2s/step - loss: 0.3512 - accuracy: 0.8664 - val_loss: 0.4363 - val_accuracy: 0.8199\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 116s 2s/step - loss: 0.3629 - accuracy: 0.8647 - val_loss: 0.3474 - val_accuracy: 0.8544\n",
            "Epoch 1/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate SVM model\n",
        "X_test_svm, y_test_svm = [], []\n",
        "\n",
        "for i in range(len(val_generator)):\n",
        "    batch_images, batch_labels = val_generator[i]\n",
        "    batch_size = batch_images.shape[0]\n",
        "    flattened_images = batch_images.reshape((batch_size, -1))\n",
        "    X_test_svm.append(flattened_images)\n",
        "    y_test_svm.append(np.argmax(batch_labels, axis=1))\n",
        "\n",
        "X_test_svm = np.vstack(X_test_svm)\n",
        "y_test_svm = np.concatenate(y_test_svm)\n",
        "\n",
        "svm_predictions = svm_model.predict(X_test_svm)\n",
        "svm_accuracy = accuracy_score(y_test_svm, svm_predictions)\n",
        "svm_f1 = f1_score(y_test_svm, svm_predictions, average='weighted')\n",
        "svm_cm = confusion_matrix(y_test_svm, svm_predictions)\n"
      ],
      "metadata": {
        "id": "9GmdYyV-Ml-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate CNN model\n",
        "cnn_predictions = []\n",
        "cnn_true_labels = []\n",
        "\n",
        "for i in range(len(val_generator)):\n",
        "    batch_images, batch_labels = val_generator[i]\n",
        "    batch_size = batch_images.shape[0]\n",
        "\n",
        "    # Get true labels\n",
        "    cnn_true_labels.extend(np.argmax(batch_labels, axis=1))\n",
        "\n",
        "    # Get predictions\n",
        "    batch_predictions = cnn_model.predict(batch_images)\n",
        "    cnn_predictions.extend(np.argmax(batch_predictions, axis=1))\n",
        "\n",
        "cnn_f1 = f1_score(cnn_true_labels, cnn_predictions, average='weighted')\n",
        "cnn_eval = cnn_model.evaluate(val_generator)\n",
        "cnn_cm = confusion_matrix(cnn_true_labels, cnn_predictions)\n"
      ],
      "metadata": {
        "id": "jh7JheDBMsGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Evaluate ViT model\n",
        "# vit_eval = vit_model.evaluate(val_generator)\n",
        "# vit_f1 = f1_score(np.argmax(val_generator.labels, axis=1), np.argmax(vit_model.predict(val_generator), axis=1), average='weighted')\n",
        "# vit_cm = confusion_matrix(np.argmax(val_generator.labels, axis=1), np.argmax(vit_model.predict(val_generator), axis=1))\n"
      ],
      "metadata": {
        "id": "NLsjVvbZMu8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate ViT model\n",
        "vit_predictions = []\n",
        "vit_true_labels = []\n",
        "\n",
        "for i in range(len(val_generator)):\n",
        "    batch_images, batch_labels = val_generator[i]\n",
        "    batch_size = batch_images.shape[0]\n",
        "\n",
        "    # Get true labels\n",
        "    vit_true_labels.extend(np.argmax(batch_labels, axis=1))\n",
        "\n",
        "    # Get predictions\n",
        "    batch_predictions = vit_model.predict(batch_images)\n",
        "    vit_predictions.extend(np.argmax(batch_predictions, axis=1))\n",
        "\n",
        "vit_f1 = f1_score(vit_true_labels, vit_predictions, average='weighted')\n",
        "vit_eval = vit_model.evaluate(val_generator)\n",
        "vit_cm = confusion_matrix(vit_true_labels, vit_predictions)\n"
      ],
      "metadata": {
        "id": "xnjYtuwL1h1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate ResNet model\n",
        "resnet_predictions = []\n",
        "resnet_true_labels = []\n",
        "\n",
        "for i in range(len(val_generator)):\n",
        "    batch_images, batch_labels = val_generator[i]\n",
        "    batch_size = batch_images.shape[0]\n",
        "\n",
        "    # Get true labels\n",
        "    resnet_true_labels.extend(np.argmax(batch_labels, axis=1))\n",
        "\n",
        "    # Get predictions\n",
        "    batch_predictions = resnet_model.predict(batch_images)\n",
        "    resnet_predictions.extend(np.argmax(batch_predictions, axis=1))\n",
        "\n",
        "resnet_f1 = f1_score(resnet_true_labels, resnet_predictions, average='weighted')\n",
        "resnet_eval = resnet_model.evaluate(val_generator)\n",
        "resnet_cm = confusion_matrix(resnet_true_labels, resnet_predictions)\n"
      ],
      "metadata": {
        "id": "oWIgibXtMxV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate MobileNet model\n",
        "mobilenet_predictions = []\n",
        "mobilenet_true_labels = []\n",
        "\n",
        "for i in range(len(val_generator)):\n",
        "    batch_images, batch_labels = val_generator[i]\n",
        "    batch_size = batch_images.shape[0]\n",
        "\n",
        "    # Get true labels\n",
        "    mobilenet_true_labels.extend(np.argmax(batch_labels, axis=1))\n",
        "\n",
        "    # Get predictions\n",
        "    batch_predictions = mobilenet_model.predict(batch_images)\n",
        "    mobilenet_predictions.extend(np.argmax(batch_predictions, axis=1))\n",
        "\n",
        "mobilenet_f1 = f1_score(mobilenet_true_labels, mobilenet_predictions, average='weighted')\n",
        "mobilenet_eval = mobilenet_model.evaluate(val_generator)\n",
        "mobilenet_cm = confusion_matrix(mobilenet_true_labels, mobilenet_predictions)\n"
      ],
      "metadata": {
        "id": "STBBglaMNmdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Models Summary**"
      ],
      "metadata": {
        "id": "tpLV3KehUJx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print and compare metrics\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"SVM F1 Score:\", svm_f1)\n",
        "print(\"SVM Confusion Matrix:\\n\", svm_cm)\n"
      ],
      "metadata": {
        "id": "KCEtPb0QO3vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CNN Accuracy:\", cnn_eval[1])\n",
        "print(\"CNN F1 Score:\", cnn_f1)\n",
        "print(\"CNN Confusion Matrix:\\n\", cnn_cm)\n"
      ],
      "metadata": {
        "id": "NnWGtzDKO5g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ViT Accuracy:\", vit_eval[1])\n",
        "print(\"ViT F1 Score:\", vit_f1)\n",
        "print(\"ViT Confusion Matrix:\\n\", vit_cm)\n"
      ],
      "metadata": {
        "id": "Is33yPMhO75o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ResNet Accuracy:\", resnet_eval[1])\n",
        "print(\"ResNet F1 Score:\", resnet_f1)\n",
        "print(\"ResNet Confusion Matrix:\\n\", resnet_cm)\n"
      ],
      "metadata": {
        "id": "XPgVsPulSicc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MobileNet Accuracy:\", mobilenet_eval[1])\n",
        "print(\"MobileNet F1 Score:\", mobilenet_f1)\n",
        "print(\"MobileNet Confusion Matrix:\\n\", mobilenet_cm)\n"
      ],
      "metadata": {
        "id": "KXJVqR7LO-yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['Benign', 'Early', 'Pre', 'Pro']\n",
        "\n",
        "# Plot the svm confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(svm_cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('SVM Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7vmiljTJEkiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the cnn confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cnn_cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Simple CNN Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eYzOAf8_RuBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the Vit confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(vit_cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Vision Transformer CNN Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "efbFiluzR65w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the resnet confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(resnet_cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Residual Networks Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gm5j_CcOSDHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the mobilenet confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(mobilenet_cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('MobileNet Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZxHpuhuyTAMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing the models on a sample**"
      ],
      "metadata": {
        "id": "wJdXBc4PUQ65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an example image for prediction\n",
        "sample_image_path = '/content/WBC-Malignant-Pro-800.jpg'\n",
        "sample_image = keras.preprocessing.image.load_img(sample_image_path, target_size=IMAGE_SIZE)\n",
        "sample_image_array = keras.preprocessing.image.img_to_array(sample_image)\n",
        "sample_image_array = np.expand_dims(sample_image_array, axis=0)"
      ],
      "metadata": {
        "id": "usQQasZIPTDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions using the trained models\n",
        "flattened_sample_image_array = sample_image_array.reshape((1, -1))\n",
        "svm_pred = svm_model.predict(flattened_sample_image_array)\n",
        "cnn_pred = cnn_model.predict(sample_image_array)\n",
        "vit_pred = vit_model.predict(sample_image_array)\n",
        "resnet_pred = resnet_model.predict(sample_image_array)\n",
        "mobilenet_pred = mobilenet_model.predict(sample_image_array)\n",
        "\n",
        "# Print predictions\n",
        "print(\"SVM Predictions:\", svm_pred)\n",
        "print(\"CNN Predictions:\", cnn_pred)\n",
        "print(\"ViT Predictions:\", vit_pred)\n",
        "print(\"ResNet Predictions:\", resnet_pred)\n",
        "print(\"MobileNet Predictions:\", mobilenet_pred)"
      ],
      "metadata": {
        "id": "e65h91pnPvvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the models to a directory\n",
        "\n",
        "cnn_model.save('leukemia_classifier_cnn_model')\n",
        "vit_model.save('leukemia_classifier_vit_model')\n",
        "resnet_model.save('leukemia_classifier_resnet_model')\n",
        "mobilenet_model.save('leukemia_classifier_mobilenet_model')\n"
      ],
      "metadata": {
        "id": "L9dtBCmAzQTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Keras model architecture and weights\n",
        "\n",
        "cnn_model.save(\"leukemia_keras_cnn_model.h5\")\n",
        "vit_model.save(\"leukemia_keras_vit_model.h5\")\n",
        "resnet_model.save(\"leukemia_keras_resnet_model.h5\")\n",
        "mobilenet_model.save(\"leukemia_keras_mobilenet_model.h5\")\n"
      ],
      "metadata": {
        "id": "qsX2_Qd9TcbR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}